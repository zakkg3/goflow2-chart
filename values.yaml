global:
  storageClass: ""  # Use cluster default

# GoFlow2 Configuration - restored from original working values.yaml
goflow2:
  enabled: true
  replicaCount: 1
  
  image:
    repository: netsampler/goflow2
    tag: "v2.2.2"
    pullPolicy: IfNotPresent
  
  resources:
    requests:
      cpu: 300m
      memory: 512Mi

  
  # Flow collection settings
  config:
    # Worker settings
    workers: 8
    bufferSize: 100000
    
    # Protocol configurations
    netflow:
      enabled: true
      port: 2055
      workers: 4
    
    sflow:
      enabled: true
      port: 6343
      workers: 4
    
    ipfix:
      enabled: false
      port: 2055
      workers: 4
    
    # Kafka output configuration
    # IMPORTANT: GoFlow2 format=bin produces binary protobuf output (NOT plain binary)
    # This matches the official GoFlow2 example and works with ClickHouse Protobuf format
    kafka:
      brokers: "netflow-pipeline-kafka:9092"
      topic: "flows"
      compression: "gzip"
      batchSize: 1000
      batchTimeout: 1000
    
    # Metrics settings
    metrics:
      enabled: true
      port: 8080
      path: "/metrics"
  
  # Service configuration for flow ingestion
  service:
    type: LoadBalancer
    annotations: {}
  
  # Pod settings
  podAnnotations: {}
  # podSecurityContext:
  #   runAsNonRoot: true
  #   runAsUser: 1000
  #   fsGroup: 1000
  
  nodeSelector: {}
  tolerations: []
  affinity: {}

# Kafka Configuration - restored from working values-stg.yaml
kafka:
  enabled: true
  
  kraft:
    enabled: true
  
  # Disable Zookeeper
  zookeeper:
    enabled: false

  # Use PLAINTEXT listeners (no authentication)
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT
  
  controller:
    replicaCount: 1
    controllerOnly: false  # Combined controller+broker mode
    persistence:
      enabled: true
      size: 2Gi
    
    resources:
      requests:
        cpu: 500m
        memory: 1.5Gi
      limits:
        cpu: 3000m
        memory: 4Gi
  
  broker:
    replicaCount: 0  # Disable separate broker - controller handles both
  
  # Production-ready Kafka configuration
  extraConfig: |
    log.retention.hours=168
    log.retention.bytes=1073741824
    log.segment.bytes=1073741824
    auto.create.topics.enable=true
    default.replication.factor=1
    num.partitions=6
    compression.type=gzip
    # CRITICAL: Fix __consumer_offsets auto-creation for single-node KRaft
    offsets.topic.replication.factor=1
    offsets.topic.num.partitions=50
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1

  # Provisioning - explicitly create internal topics with correct replication
  provisioning:
    enabled: true
    topics:
      - name: __consumer_offsets
        partitions: 50
        replicationFactor: 1
        config:
          compression.type: "producer"
          cleanup.policy: "compact"
      - name: flows
        partitions: 6
        replicationFactor: 1
        config:
          retention.ms: "604800000"  # 7 days
          compression.type: "gzip"

  # Service configuration
  service:
    type: ClusterIP
    ports:
      client: 9092

# ClickHouse Configuration - staging settings
clickhouse:
  enabled: true
  
  auth:
    username: default
    password: "1234"
    database: netflow
  
  shards: 1
  replicaCount: 1
  
  persistence:
    enabled: true
    size: 5Gi
  
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi
  
  # Mount protobuf schema for Kafka engine
  extraVolumes:
    - name: protobuf-schema
      configMap:
        name: netflow-pipeline-clickhouse-clickhouse-proto
  
  extraVolumeMounts:
    - name: protobuf-schema
      mountPath: /var/lib/clickhouse/format_schemas
      readOnly: true
  
  # Init scripts based on goflow2 creation script
  initdbScripts:
    init.sh: |
      #!/bin/bash
      set -e
      echo "Initializing ClickHouse databases..."
      
      # Wait for ClickHouse to be ready
      # until clickhouse client --query "SELECT 1" > /dev/null 2>&1; do
      #   echo "Waiting for ClickHouse to be ready..."
      #   sleep 2
      # done
      
      echo "Creating databases and tables..."
      clickhouse client --user=default --password=1234 -n <<-EOSQL
      -- Create dictionaries database
      CREATE DATABASE IF NOT EXISTS dictionaries;
      
      -- Create netflow database  
      CREATE DATABASE IF NOT EXISTS netflow;
      
      -- Create protocols dictionary
      CREATE TABLE IF NOT EXISTS dictionaries.protocols
      (
          proto UInt8,
          name String,
          description String
      ) ENGINE = TinyLog;
      
      -- Insert protocol data
      INSERT INTO dictionaries.protocols VALUES
      (1, 'ICMP', 'Internet Control Message Protocol'),
      (6, 'TCP', 'Transmission Control Protocol'),  
      (17, 'UDP', 'User Datagram Protocol');
      
      -- Create flows table (Kafka engine) - exact match to GoFlow2 example
      CREATE TABLE IF NOT EXISTS netflow.flows
      (
          time_received_ns UInt64,
          time_flow_start_ns UInt64,
          sequence_num UInt32,
          sampling_rate UInt64,
          sampler_address FixedString(16),
          src_addr FixedString(16),
          dst_addr FixedString(16),
          src_as UInt32,
          dst_as UInt32,
          etype UInt32,
          proto UInt32,
          src_port UInt32,
          dst_port UInt32,
          bytes UInt64,
          packets UInt64
      ) ENGINE = Kafka()
      SETTINGS
          kafka_broker_list = 'netflow-pipeline-kafka:9092',
          kafka_num_consumers = 1,
          kafka_topic_list = 'flows',
          kafka_group_name = 'clickhouse',
          kafka_format = 'Protobuf',
          kafka_schema = 'flow.proto:FlowMessage';
      
      -- Create flows_raw table (MergeTree)  
      CREATE TABLE IF NOT EXISTS netflow.flows_raw
      (
          date Date,
          time_inserted_ns DateTime64(9),
          time_received_ns DateTime64(9),
          time_flow_start_ns DateTime64(9),
          sequence_num UInt32,
          sampling_rate UInt64,
          sampler_address FixedString(16),
          src_addr FixedString(16),
          dst_addr FixedString(16),
          src_as UInt32,
          dst_as UInt32,
          etype UInt32,
          proto UInt32,
          src_port UInt32,
          dst_port UInt32,
          bytes UInt64,
          packets UInt64
      ) ENGINE = MergeTree()
      PARTITION BY date
      ORDER BY time_received_ns;
      
      -- Create materialized view with proper type conversions
      CREATE MATERIALIZED VIEW IF NOT EXISTS netflow.flows_raw_view TO netflow.flows_raw
      AS SELECT
          toDate(now()) AS date,
          now() AS time_inserted_ns,
          fromUnixTimestamp64Nano(time_received_ns) AS time_received_ns,
          fromUnixTimestamp64Nano(time_flow_start_ns) AS time_flow_start_ns,
          sequence_num,
          sampling_rate,
          sampler_address,
          src_addr,
          dst_addr,
          src_as,
          dst_as,
          etype,
          proto,
          src_port,
          dst_port,
          bytes,
          packets
         FROM netflow.flows;
      EOSQL
      
      echo "Database initialization completed successfully!"
  

# Dashboard ConfigMap generation  
dashboardsConfigmap:
  enable: true

# Grafana Configuration - staging with LoadBalancer
grafana:
  enabled: true
  
  adminUser: admin
  adminPassword: admin
  
  service:
    type: LoadBalancer
    port: 80
  
  persistence:
    enabled: true
    size: 5Gi
  
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  # Grafana plugins for ClickHouse
  plugins:
    - vertamedia-clickhouse-datasource
    - grafana-piechart-panel
  
  # Pre-configure ClickHouse datasource
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: ClickHouse
          type: vertamedia-clickhouse-datasource
          access: proxy
          url: http://netflow-pipeline-clickhouse:8123
          basicAuth: true
          basicAuthUser: default
          isDefault: true
          secureJsonData:
            basicAuthPassword: "1234"
          jsonData:
            defaultDatabase: netflow
        - name: Prometheus
          type: prometheus
          access: proxy
          url: http://netflow-pipeline-prometheus-server:80
          isDefault: false
  
  # Dashboard provisioning
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'netflow'
          orgId: 1
          folder: 'NetFlow'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 10
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/netflow
  
  # Enable sidecar to automatically discover dashboards with label
  sidecar:
    datasources:
      enabled: true
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      folderAnnotation: grafana_folder
      provider:
        allowUiUpdates: true
        foldersFromFilesStructure: true

  ingress:
    enabled: true
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      netcenter.hpc.ethz.ch/from-host: enabled
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /
    hosts:
      - netflow.eu-stg.hpc.ethz.ch
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    tls: 
     - secretName: netflow-hpc-ethz-ch
       hosts:
         - netflow.eu-stg.hpc.ethz.ch

# Simple Prometheus for GoFlow2 metrics
prometheus:
  enabled: true
  
  server:
    persistentVolume:
      enabled: true
      size: 5Gi
    
    retention: "7d"
    
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
  
  # Disable components we don't need
  alertmanager:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  
  # ServiceMonitor configuration
  serviceMonitorSelectorNilUsesHelmValues: false
  
  serviceMonitor:
    enabled: true
    labels: {}
    interval: "30s"
    scrapeTimeout: "10s"
  
  serverFiles:
    prometheus.yml:
      scrape_configs:
        - job_name: goflow2
          static_configs:
            - targets:
              - netflow-pipeline-goflow2:8080